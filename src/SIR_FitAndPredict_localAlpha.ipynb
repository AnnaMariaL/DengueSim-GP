{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Data: Fit & Predict 2007 to 2019\n",
    "\n",
    "\n",
    "## Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SIR_gp import * #class implementation of the GP\n",
    "from SIR_plot import * #functions for plotting of GP outputs \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from emukit.core import ParameterSpace, ContinuousParameter #emukit for LHS\n",
    "from emukit.core.initial_designs.latin_design import LatinDesign #emukit for LHS\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #only if you are really sure about this one "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = \"../data/empirical/OpenDengue-epidemics-reformat.txt\" #empirical outbreak data \n",
    "PATH_GP_TRAIN = \"../GPs/imax/sim-training-maxIncidence-round15.txt\" #GP: path training data \n",
    "PATH_GP_TEST = \"../data/sim/imax-duration/DD-AML-test-LHS-10000-condSim-logDuration.txt\" #GP: path test data \n",
    "PATH_GP_MODEL = \"../GPs/imax/maxIncidence-round15-snap3.pth\" #GP: path model snapshot \n",
    "\n",
    "PARAM_RANGES = ParameterSpace([ #parameter ranges, without alphaRest, including correction factor\n",
    "    ContinuousParameter(\"alphaAmp\", 0, 1),\n",
    "    ContinuousParameter(\"alphaShift\", 0, 1),\n",
    "    ContinuousParameter(\"infTicksCount\", 4, 6),\n",
    "    ContinuousParameter(\"avgVisitsCount\", 1, 5),\n",
    "    ContinuousParameter(\"pVisits\", 0.05, 0.95),\n",
    "    ContinuousParameter(\"propSocialVisits\", 0, 1),\n",
    "    ContinuousParameter(\"locPerSGCount\", 1, 20),\n",
    "    ContinuousParameter(\"correctionFactor\", 0, 0.1)\n",
    "])\n",
    "\n",
    "PARAM_RANGES_ALPHA = [0, 0.03] #define parameter range & number of steps for alphaRest \n",
    "PARAM_STEPS_ALPHA = 50\n",
    "\n",
    "N_SAMPLE_LHS = 25000 #samples to draw from LHS \n",
    "RANDOM_STATE_SEED = 42 #random seed (LHS sample, test/train split)\n",
    "\n",
    "PROP_FIT = 0.67 #proportion of data used for fitting\n",
    "\n",
    "TOP_X = 250 #store TOP_X parameter combinations with lowest RMSE\n",
    "N_SHUFFLE = 1000 #number of shuffling iterations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_min_count_epidemics(df, mincount):\n",
    "    #filter_min_count_eppidemics: filter for municipalities with at least mincount outbreaks\n",
    "    outbreak_counts = df['ADM2_PCODE'].value_counts()\n",
    "    municipalities_ID = outbreak_counts[outbreak_counts >= mincount].index\n",
    "    df_filtered = df[df['ADM2_PCODE'].isin(municipalities_ID)]\n",
    "    return df_filtered\n",
    "\n",
    "def split_dataframe(df, p):\n",
    "    #split_dataframe: split df into two df (group = municipality)\n",
    "    #p --> a, 1 - p --> b\n",
    "    groups = df.groupby('ADM2_PCODE')\n",
    "\n",
    "    df_a = pd.DataFrame() #generate empty data frames\n",
    "    df_b = pd.DataFrame()\n",
    "    \n",
    "    for name, group in groups: #for each group \n",
    "        num_outbreaks = len(group) #determine number of outbreaks \n",
    "        indices = np.arange(num_outbreaks) #split randomly into two groups\n",
    "        np.random.shuffle(indices)\n",
    "        split_index = int(p * num_outbreaks) #set split index\n",
    "        \n",
    "        df_a = pd.concat([df_a, group.iloc[indices[:split_index]]])\n",
    "        df_b = pd.concat([df_b, group.iloc[indices[split_index:]]])\n",
    "    \n",
    "    # Reset indices\n",
    "    df_a = df_a.reset_index(drop=True)\n",
    "    df_b = df_b.reset_index(drop=True)\n",
    "    \n",
    "    return df_a, df_b\n",
    "\n",
    "def format_data(LHS_sample, epidemics, epidemic_id):\n",
    "    #format_data: generate torch for GP prediction, with epidemic-specific phaseShift parameters \n",
    "    res = LHS_sample.copy()\n",
    "    epidemic_timepoint = epidemics['t'].iloc[epidemic_id] #extract epidemic-specific data \n",
    "    phaseShift = res[:, 2] + epidemic_timepoint/365\n",
    "    phaseShift_scaled = phaseShift % 1\n",
    "    res[: ,2] = phaseShift_scaled #calulate overall phase shift\n",
    "    res = torch.from_numpy(res).float().contiguous() #reformat\n",
    "    return(res)\n",
    "\n",
    "def calculate_rmse(epidemics, predictions, model_type):\n",
    "    #calculate_rmse: calculate RMSE; epidemics = df with empirical outbreaks; predictions = GP predictions\n",
    "    #note different shapes --> np.tile()\n",
    "    if model_type == \"maxIncidence\": #extract summary stat \n",
    "        observed = epidemics['imax'].values \n",
    "    if model_type == \"duration\":\n",
    "        observed = epidemics['duration'].values \n",
    "        observed = np.log10(observed)\n",
    "    \n",
    "    observed = observed.reshape(1, -1)  # reshape observed values \n",
    "    observed = np.tile(observed, (predictions.shape[0], 1)) #account for different shape\n",
    "    rmse = np.sqrt(np.mean((observed - predictions) ** 2, axis=1))  # calculate RMSE\n",
    "    return rmse\n",
    "\n",
    "def predict_var(LHS_sample, epidemics, GP_model, verbose = False):\n",
    "    #predict_VAR: formate LHS_sample parameters with information from epidemics and predict using GP_model\n",
    "    if GP_model.model_type == \"maxIncidence\": #extract summary stat \n",
    "        observed = epidemics['imax'].values \n",
    "    if GP_model.model_type == \"duration\":\n",
    "        observed = epidemics['duration'].values \n",
    "        observed = np.log10(observed)\n",
    "\n",
    "    last_column = LHS_sample[:, -1].reshape(-1, 1)  # correction factor: reshaped to (num_predictions, 1)\n",
    "\n",
    "    num_predictions = LHS_sample.shape[0] #number of predictions \n",
    "    num_epidemics = len(observed) #number of epidemics\n",
    "\n",
    "    res = np.empty((num_predictions, num_epidemics)) #generate empty numpy array\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(f\"Predicting {num_predictions} points for {num_epidemics} epidemics - please be patient ...\")\n",
    "\n",
    "    for i in range(num_epidemics): #for each epidemic\n",
    "        if verbose == True:\n",
    "            if i % 10 == 0:\n",
    "                print(f\"{i} out of {num_epidemics} epidemics done\")\n",
    "\n",
    "        iter_points = format_data(LHS_sample=LHS_sample[:,:-1], epidemics=epidemics, epidemic_id=i) #prep data \n",
    "        iter_pred, _, _ = GP_model.predict_ys(parsed_data = iter_points) #make predictions\n",
    "            \n",
    "        if GP_model.model_type == \"maxIncidence\": #store predictions; adjust for correction factor if estimate == imax\n",
    "            res[:, i] = iter_pred.numpy().flatten() * last_column.flatten()  \n",
    "        if GP_model.model_type == \"duration\":\n",
    "            res[:, i] = iter_pred.numpy().flatten()\n",
    "\n",
    "    if GP_model.model_type == \"maxIncidence\": #clip to range\n",
    "        res = np.clip(res, 0.0, 1.0)\n",
    "    \n",
    "    if GP_model.model_type == \"duration\":\n",
    "        res = np.clip(res, 0.0, 3.0)\n",
    "\n",
    "    return res #nrow(LHS_sample) x len(observed)  \n",
    "\n",
    "def shuffle_var(emp_df, f_alpha_df, f_candidate_LHS, n_iter, shuffle_var, GP_model): \n",
    "   #shuffle_var: conduct perumtation tests\n",
    "      #emp_df : empirical data frame\n",
    "      #f_alpha_df: alphaRest estimates per municipality\n",
    "      #f_candidate_LHS: input parameters \n",
    "      #n_iter: iteration count for reshuffling\n",
    "      #shuffle_var: shuffling type; 3 options\n",
    "         #day: time of outbreak\n",
    "         #municipality: municipality\n",
    "         #both: day & municipality \n",
    "      #GP_model: GP model used for predictions \n",
    "   f_df = emp_df.copy()\n",
    "   shuffle_stat = [] #create empty list for corr. coeff\n",
    "\n",
    "   for i in range(n_iter): #conduct shuffling \n",
    "        f_predictions = pd.DataFrame()\n",
    "\n",
    "        if shuffle_var == 'day':\n",
    "            f_df['t'] = np.random.permutation(f_df['t'].values)\n",
    "\n",
    "        if shuffle_var == 'municipality':\n",
    "            f_df['ADM2_PCODE'] = np.random.permutation(f_df['ADM2_PCODE'].values)\n",
    "\n",
    "        if shuffle_var == 'both':\n",
    "            f_df['t'] = np.random.permutation(f_df['t'].values)\n",
    "            f_df['ADM2_PCODE'] = np.random.permutation(f_df['ADM2_PCODE'].values)\n",
    "            \n",
    "        for m in f_alpha_df['municipality']: #for each municipality\n",
    "\n",
    "            f_df_m = f_df.loc[f_df['ADM2_PCODE'] == m] #subset epidemics\n",
    "            f_df_m = f_df_m.reset_index(drop = True)\n",
    "         \n",
    "            f_alpha_m = f_alpha_df.loc[f_alpha_df['municipality'] == m] #retrieve alphaRest values\n",
    "            f_alpha_m = f_alpha_m['alphaRest']\n",
    "         \n",
    "            f_params = np.hstack((f_alpha_m, f_candidate_LHS)) #create parameter array\n",
    "            f_params = f_params.reshape(1, 9)\n",
    "         \n",
    "            f_pred = predict_var(LHS_sample=f_params, epidemics=f_df_m, GP_model=GP_model) #predict\n",
    "            f_pred_df = pd.DataFrame(f_pred.flatten(), columns=['pred']) #attach predictions \n",
    "            f_combinded_df = pd.concat((f_df_m, f_pred_df), axis = 1) \n",
    "      \n",
    "            f_predictions = pd.concat([f_predictions, f_combinded_df], ignore_index=True) #store municipality results \n",
    "\n",
    "        iter_stat = spearmanr(f_predictions['imax'], f_predictions['pred']).statistic #calculate corr.coeff\n",
    "        shuffle_stat.append(iter_stat)\n",
    "   return shuffle_stat #return n_iter long list of correlation coefficients\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emp Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1186, 5)\n",
      "(737, 5)\n",
      "(449, 5)\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE_SEED) #for reproducibility\n",
    "\n",
    "epidemics = pd.read_csv(PATH_DATA, sep=\"\\t\", header=0) #read all epidemics\n",
    "epidemics_filtered = filter_min_count_epidemics(epidemics, 3) #filter for municipalities with at least 3 outbreaks \n",
    "df_fit, df_pred = split_dataframe(epidemics_filtered, PROP_FIT) #split into two data frames: fit, prediction \n",
    "municipalities = df_fit['ADM2_PCODE'].unique() #unique municipalities \n",
    "\n",
    "print(epidemics_filtered.shape)\n",
    "print(df_fit.shape) #df_fit purpose: determine best fitting parameter combination\n",
    "print(df_pred.shape) #df_pred purpose: assess predictive power | best fitting parameter combination \n",
    "print(len(municipalities))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 8)\n",
      "(50, 1)\n",
      "(1250000, 9)\n"
     ]
    }
   ],
   "source": [
    "candidates_LHS = LatinDesign(PARAM_RANGES).get_samples(N_SAMPLE_LHS) #LHS sample\n",
    "candidates_alpha = np.linspace(PARAM_RANGES_ALPHA[0], PARAM_RANGES_ALPHA[1], PARAM_STEPS_ALPHA) #alphaRest sample \n",
    "candidates_alpha = candidates_alpha.reshape(candidates_alpha.shape[0], 1)\n",
    "\n",
    "candidates = candidates_LHS.repeat(candidates_alpha.shape[0], axis=0) #repeat LHS sample \n",
    "candidates_alpha_tiled = np.tile(candidates_alpha, (candidates_LHS.shape[0],1)) #tile alphaRest sample \n",
    "candidates = np.hstack((candidates_alpha_tiled, candidates)) #concatenate numpy arrays: alphaRest in 1st column \n",
    "\n",
    "print(candidates_LHS.shape)\n",
    "print(candidates_alpha.shape)\n",
    "print(candidates.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load: GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Loss: -1.7720226049423218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04204195387554575"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myGP = SIR_GP(training_data=PATH_GP_TRAIN, model_type=\"maxIncidence\") #load the GP surrogate model\n",
    "myGP.load(filename=PATH_GP_MODEL)\n",
    "myGP.get_rmse(PATH_GP_TEST) #sanity check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1250000 points for 737 epidemics - please be patient ...\n",
      "0 out of 737 epidemics done\n",
      "10 out of 737 epidemics done\n",
      "20 out of 737 epidemics done\n",
      "30 out of 737 epidemics done\n",
      "40 out of 737 epidemics done\n",
      "50 out of 737 epidemics done\n",
      "60 out of 737 epidemics done\n",
      "70 out of 737 epidemics done\n",
      "80 out of 737 epidemics done\n",
      "90 out of 737 epidemics done\n",
      "100 out of 737 epidemics done\n",
      "110 out of 737 epidemics done\n",
      "120 out of 737 epidemics done\n",
      "130 out of 737 epidemics done\n",
      "140 out of 737 epidemics done\n",
      "150 out of 737 epidemics done\n",
      "160 out of 737 epidemics done\n",
      "170 out of 737 epidemics done\n",
      "180 out of 737 epidemics done\n",
      "190 out of 737 epidemics done\n",
      "200 out of 737 epidemics done\n",
      "210 out of 737 epidemics done\n",
      "220 out of 737 epidemics done\n",
      "230 out of 737 epidemics done\n",
      "240 out of 737 epidemics done\n",
      "250 out of 737 epidemics done\n",
      "260 out of 737 epidemics done\n",
      "270 out of 737 epidemics done\n",
      "280 out of 737 epidemics done\n",
      "290 out of 737 epidemics done\n",
      "300 out of 737 epidemics done\n",
      "310 out of 737 epidemics done\n",
      "320 out of 737 epidemics done\n",
      "330 out of 737 epidemics done\n",
      "340 out of 737 epidemics done\n",
      "350 out of 737 epidemics done\n",
      "360 out of 737 epidemics done\n",
      "370 out of 737 epidemics done\n",
      "380 out of 737 epidemics done\n",
      "390 out of 737 epidemics done\n",
      "400 out of 737 epidemics done\n",
      "410 out of 737 epidemics done\n",
      "420 out of 737 epidemics done\n",
      "430 out of 737 epidemics done\n",
      "440 out of 737 epidemics done\n",
      "450 out of 737 epidemics done\n",
      "460 out of 737 epidemics done\n",
      "470 out of 737 epidemics done\n",
      "480 out of 737 epidemics done\n",
      "490 out of 737 epidemics done\n",
      "500 out of 737 epidemics done\n",
      "510 out of 737 epidemics done\n",
      "520 out of 737 epidemics done\n",
      "530 out of 737 epidemics done\n",
      "540 out of 737 epidemics done\n",
      "550 out of 737 epidemics done\n",
      "560 out of 737 epidemics done\n",
      "570 out of 737 epidemics done\n",
      "580 out of 737 epidemics done\n",
      "590 out of 737 epidemics done\n",
      "600 out of 737 epidemics done\n",
      "610 out of 737 epidemics done\n",
      "620 out of 737 epidemics done\n",
      "630 out of 737 epidemics done\n",
      "640 out of 737 epidemics done\n",
      "650 out of 737 epidemics done\n",
      "660 out of 737 epidemics done\n",
      "670 out of 737 epidemics done\n",
      "680 out of 737 epidemics done\n",
      "690 out of 737 epidemics done\n",
      "700 out of 737 epidemics done\n",
      "710 out of 737 epidemics done\n",
      "720 out of 737 epidemics done\n",
      "730 out of 737 epidemics done\n",
      "(1250000, 737)\n"
     ]
    }
   ],
   "source": [
    "pred = predict_var(candidates, GP_model=myGP, epidemics=df_fit, verbose=True) #perform N_SAMPLE_LHS x N_ALPHAREST predictions for all epidemics in df_fit\n",
    "print(pred.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate RMSE per municipality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250000, 173)\n"
     ]
    }
   ],
   "source": [
    "num_municipalities = len(municipalities) #number of municipalities \n",
    "num_predictions = candidates.shape[0] #number of predictions \n",
    "rmse_municipality = np.empty((num_predictions, num_municipalities))\n",
    "\n",
    "for m in range(num_municipalities):\n",
    "    m_index = df_fit.index[df_fit['ADM2_PCODE'] == municipalities[m]].tolist() #ID with specific municipality\n",
    "    rmse_iter = calculate_rmse(epidemics=df_fit.iloc[m_index], predictions=pred[:, m_index], model_type=myGP.model_type)\n",
    "    rmse_municipality[:, m] = rmse_iter.flatten()\n",
    "\n",
    "print(rmse_municipality.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## argmin(alpha) per municipality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 173)\n",
      "(25000, 173)\n"
     ]
    }
   ],
   "source": [
    "alpha_municipalitites = np.empty((N_SAMPLE_LHS, num_municipalities)) #set up empty numpy arrays \n",
    "min_rmse_municipalities = np.empty((N_SAMPLE_LHS, num_municipalities))\n",
    "\n",
    "for m in range(num_municipalities): #for each municipality\n",
    "        alpha_m = [] #create empty lists \n",
    "        rmse_m = []\n",
    "        for i in range(0, num_predictions, PARAM_STEPS_ALPHA): #for each \"block\" of input params (i.e., same input domain, different alphaRest values)\n",
    "                rmse_L_m = rmse_municipality[i:(i + PARAM_STEPS_ALPHA), m] #subset rmse_municipality to block\n",
    "                min_rmse_index = np.argmin(rmse_L_m) #find index & value of smallest RMSE within block\n",
    "                min_rmse = np.min(rmse_L_m)\n",
    "\n",
    "                alpha_L = candidates_alpha[min_rmse_index] #store corresponding alpha \n",
    "                alpha_m.append(alpha_L)\n",
    "                rmse_m.append(min_rmse) #store smallest RMSE value within block\n",
    "\n",
    "        alpha_m = np.array(alpha_m) #reformat\n",
    "        rmse_m = np.array(rmse_m)\n",
    "\n",
    "        alpha_municipalitites[:,m] = alpha_m.flatten()\n",
    "        min_rmse_municipalities[:,m] = rmse_m.flatten()\n",
    "\n",
    "print(alpha_municipalitites.shape) #LHS x N_municipalities\n",
    "print(min_rmse_municipalities.shape) #LHS x N_municipalities\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## argmin(LHS) for df_fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000, 8)\n",
      "[[0.15758  0.57682  4.6714   4.3892   0.465818 0.99326  4.54958  0.035294]]\n",
      "<bound method NDFrame.describe of     municipality  alphaRest\n",
      "0        CO05001   0.005510\n",
      "1        CO05045   0.003673\n",
      "2        CO05051   0.003673\n",
      "3        CO05079   0.003673\n",
      "4        CO05088   0.003673\n",
      "..           ...        ...\n",
      "168      CO86865   0.015306\n",
      "169      CO91001   0.029388\n",
      "170      CO95001   0.007347\n",
      "171      CO97001   0.012857\n",
      "172      CO99773   0.006735\n",
      "\n",
      "[173 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rmse_sums = np.sum(min_rmse_municipalities, axis=1) #build RMSE sums across all municipalities \n",
    "bestFitID = np.argmin(rmse_sums) #ID of best fit = lowest RMSE sum \n",
    "\n",
    "print(rmse_sums.shape) #N_LHSE\n",
    "print(candidates_LHS.shape)  #N_LHSE x 8 (no alpha)\n",
    "print(candidates_LHS[bestFitID:(bestFitID+1),:]) #best fit \n",
    "\n",
    "alphaData={ #store argmin(alpha) for all municipalities\n",
    "    'municipality': municipalities,\n",
    "    'alphaRest' : alpha_municipalitites[bestFitID]\n",
    "}\n",
    "\n",
    "alpha_df = pd.DataFrame(alphaData)\n",
    "print(alpha_df.describe)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top X candidates for df_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 8)\n",
      "<bound method NDFrame.describe of      alphaAmp  alphaShift  infTicksCount  avgVisitsCount   pVisits  \\\n",
      "0     0.15758     0.57682        4.67140         4.38920  0.465818   \n",
      "1     0.05066     0.90394        4.04916         1.77752  0.775670   \n",
      "2     0.08278     0.63590        4.66916         4.69256  0.941162   \n",
      "3     0.00758     0.52826        4.23660         1.65576  0.699134   \n",
      "4     0.06206     0.95290        5.69348         1.73160  0.489758   \n",
      "..        ...         ...            ...             ...       ...   \n",
      "245   0.07866     0.58714        5.24924         2.88104  0.273434   \n",
      "246   0.07650     0.21110        4.19628         1.35864  0.646934   \n",
      "247   0.07746     0.60634        5.35988         1.88936  0.423626   \n",
      "248   0.12554     0.47910        4.07788         1.75128  0.125006   \n",
      "249   0.02158     0.89882        4.78516         2.46808  0.159746   \n",
      "\n",
      "     propSocialVisits  locPerSGCount  correctionFactor  \n",
      "0             0.99326        4.54958          0.035294  \n",
      "1             0.91262       14.34902          0.037514  \n",
      "2             0.99038       14.50938          0.020666  \n",
      "3             0.80542        8.40962          0.028914  \n",
      "4             0.98722        4.09434          0.069866  \n",
      "..                ...            ...               ...  \n",
      "245           0.54274       14.13394          0.026354  \n",
      "246           0.20234        7.52574          0.033410  \n",
      "247           0.92426        9.61042          0.043154  \n",
      "248           0.14090        7.07202          0.023426  \n",
      "249           0.21098        3.05466          0.027398  \n",
      "\n",
      "[250 rows x 8 columns]>\n"
     ]
    }
   ],
   "source": [
    "top_indices = np.argsort(rmse_sums)[:TOP_X] #TOP_X LHS with lowest RMSE\n",
    "\n",
    "best_fits = candidates_LHS[top_indices, :] #get best TOP_X fits\n",
    "print(best_fits.shape)  # Top X best fits\n",
    "best_fits_df = pd.DataFrame(best_fits, columns=PARAM_RANGES.parameter_names)\n",
    "print(best_fits_df.describe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2)) #store RMSE histogram\n",
    "plt.hist(np.log10(rmse_sums), bins=1000)\n",
    "plt.axvline(x = np.log10(np.max(rmse_sums[top_indices])), color = 'black', lw = 0.5, linestyle = 'dashed')\n",
    "plt.title('Histogram of RMSE values')\n",
    "plt.xlabel('log10(RMSE)')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(f'../FitPredict/{myGP.model_type}-RMSE.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.describe of     municipality  alphaRest      rmse  rank\n",
      "0        CO05001   0.005510  0.556908     1\n",
      "1        CO05045   0.003673  0.556908     1\n",
      "2        CO05051   0.003673  0.556908     1\n",
      "3        CO05079   0.003673  0.556908     1\n",
      "4        CO05088   0.003673  0.556908     1\n",
      "..           ...        ...       ...   ...\n",
      "168      CO86865   0.009184  0.570453   250\n",
      "169      CO91001   0.015306  0.570453   250\n",
      "170      CO95001   0.006735  0.570453   250\n",
      "171      CO97001   0.011020  0.570453   250\n",
      "172      CO99773   0.006735  0.570453   250\n",
      "\n",
      "[43250 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(top_indices)):\n",
    "\n",
    "    alphaData_iter = { #alphaRest estimates for top X candidates \n",
    "        'municipality': municipalities,\n",
    "        'alphaRest': alpha_municipalitites[top_indices[i]],\n",
    "    }\n",
    "    alpha_df_iter = pd.DataFrame(alphaData_iter)\n",
    "    alpha_df_iter['rmse'] =rmse_sums[top_indices[i]]\n",
    "    alpha_df_iter['rank'] = i + 1 \n",
    "\n",
    "    if i == 0 :\n",
    "        alpha_df_topX = alpha_df_iter\n",
    "    else:\n",
    "        alpha_df_topX = pd.concat([alpha_df_topX, alpha_df_iter])\n",
    "\n",
    "print(alpha_df_topX.describe)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictions = pd.DataFrame()\n",
    "\n",
    "for m in alpha_df['municipality']:\n",
    "    df_pred_m = df_pred.loc[df_pred['ADM2_PCODE'] == m] #extract municipality-specific epidemics\n",
    "    df_pred_m = df_pred_m.reset_index(drop = True)\n",
    "    alpha_m = alpha_df.loc[alpha_df['municipality'] == m ]\n",
    "    alpha_m = alpha_m['alphaRest'] #extract municipality-specific alphaRest estimates \n",
    "    pred_params = np.hstack((alpha_m, candidates_LHS[bestFitID]))\n",
    "    pred_params = pred_params.reshape(1, 9)\n",
    "    pred_m = predict_var(LHS_sample=pred_params, epidemics=df_pred_m, GP_model=myGP, verbose=False)\n",
    "    pred_m_df = pd.DataFrame(pred_m.flatten(), columns=['pred'])\n",
    "    combined_df = pd.concat([df_pred_m, pred_m_df], axis = 1)\n",
    "    my_predictions = pd.concat([my_predictions, combined_df], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "permut_day = shuffle_var(emp_df=df_pred, f_alpha_df=alpha_df, f_candidate_LHS=candidates_LHS[bestFitID], n_iter=N_SHUFFLE, shuffle_var='day', GP_model=myGP)\n",
    "permut_municipality = shuffle_var(emp_df=df_pred, f_alpha_df=alpha_df, f_candidate_LHS=candidates_LHS[bestFitID], n_iter=N_SHUFFLE, shuffle_var='municipality', GP_model=myGP)\n",
    "permut_both = shuffle_var(emp_df=df_pred, f_alpha_df=alpha_df, f_candidate_LHS=candidates_LHS[bestFitID], n_iter=N_SHUFFLE, shuffle_var='both', GP_model=myGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "permut_df  = pd.DataFrame({'day': permut_day,\n",
    "                           'municipality': permut_municipality,\n",
    "                           'both': permut_both})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean        0.476932\n",
       "std         0.009892\n",
       "min         0.442987\n",
       "25%         0.470417\n",
       "50%         0.476721\n",
       "75%         0.483508\n",
       "max         0.505091\n",
       "Name: day, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permut_df['day'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean       -0.024904\n",
       "std         0.046158\n",
       "min        -0.198912\n",
       "25%        -0.058474\n",
       "50%        -0.023948\n",
       "75%         0.008134\n",
       "max         0.101897\n",
       "Name: municipality, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permut_df['municipality'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean        0.000788\n",
       "std         0.049002\n",
       "min        -0.172837\n",
       "25%        -0.031850\n",
       "50%        -0.000027\n",
       "75%         0.034127\n",
       "max         0.168697\n",
       "Name: both, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permut_df['both'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4581849943199259"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(my_predictions['imax'], my_predictions['pred']).statistic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictions.to_csv(f'../FitPredict/{myGP.model_type}-pred-localAlpha.csv', index=False, header=True, sep='\\t') #predictions\n",
    "alpha_df.to_csv(f'../FitPredict/{myGP.model_type}-alphaRest-fit-localAlpha.csv', index=False, header=True, sep='\\t') #best fitting alphaRest parameters\n",
    "alpha_df_topX.to_csv(f'../FitPredict/{myGP.model_type}-alphaRest-top{TOP_X}-localAlpha.csv', index=False, header=True, sep='\\t') #top <TOP_X> alphaRest parameters\n",
    "best_fits_df.to_csv(f'../FitPredict/{myGP.model_type}-params-top{TOP_X}-localAlpha.csv', index=False, header=True, sep=\"\\t\") #top <TOP_X> parameters\n",
    "permut_df.to_csv(f'../FitPredict/{myGP.model_type}-permutations-localAlpha.csv', index=False, header=True, sep='\\t') #permutation results "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
