{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Parameter Exploration with Gaussian Processes\n",
    "\n",
    "This notebook explores which parameter combinations best reproduce the observed dengue incidence in Colombia, using a pre-trained Gaussian Process (GP) emulator for the maximum incidence.\n",
    "\n",
    "Key points of the analysis:\n",
    "\n",
    "* **Municipality-Specific Calibration:** Municipality-level heterogeneity in transmission is captured via municipality-specific average infectivities. All other model parameters are held constant across municipalities (except for the timing of the first case, which is addressed separately).\n",
    "\n",
    "* **Data Splitting for Calibration and Testing:** Among 173 municipalities with at least three detected epidemics (1,186 outbreaks total), data are split within each municipality into 67% for calibration (737 outbreaks total) and 33% for testing (449 outbreaks total).\n",
    "\n",
    "* **Goal:** Identify the parameter combinations that results in the best agreement between GP-predicted maximum incidence values and the empirical dengue outbreak data.\n",
    "\n",
    "**Note:** The GP implementation uses slightly different variable names than those in the manuscript. The table below shows the correspondence.\n",
    "\n",
    "| GP Variable        | Manuscript Term           | Description                                                                 |\n",
    "|--------------------|---------------------------|------------------------------------------------------------------------------|\n",
    "| `alphaRest`        | Average infectivity        | Baseline probability of infection per day.                                   |\n",
    "| `alphaAmp`         | Seasonality strength       | Scaling factor controlling seasonal variation in infection probability.      |\n",
    "| `alphaShift`       | First case timing          | Timing of the first case relative to the seasonal peak in infection probability. |\n",
    "| `infTicksCount`    | Infectious period          | Average number of days an individual remains infectious.                     |\n",
    "| `avgVisitsCount`   | Average mobility           | Average number of daily visits per individual.                               |\n",
    "| `pVisits`          | Mobility skewness          | Parameter controlling variability of daily visits.                           |\n",
    "| `propSocialVisits` | Social structure           | Probability that a visit occurs within a family cluster.                     |\n",
    "| `locPerSGCount`    | Family cluster size        | Number of locations per family cluster.                                      |\n",
    "\n",
    "--- \n",
    "\n",
    "## Setup\n",
    "\n",
    "### Imports\n",
    "\n",
    "This chunk prepares the environment for calibrating the GP emulator to empirical dengue outbreak data:\n",
    "\n",
    "1. **`SIR_gp`**: Imports the GP emulator class. \n",
    "2. **Emukit**: Provides tools for parameter space definitions (`ParameterSpace`, `ContinuousParameter`) and Latin Hypercube Sampling (`LatinDesign`) to explore the parameter space efficiently.\n",
    "4. **Spearman correlation**: `scipy.stats.spearmanr` will be used to compare GP predictions with empirical data.\n",
    "5. **Warnings**: Suppressed to keep notebook output clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GP emulator class and required packages\n",
    "from SIR_gp import SIR_GP  # assuming SIR_GP is the class name in SIR_gp.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Emukit for parameter space definition and Latin Hypercube Sampling\n",
    "from emukit.core import ParameterSpace, ContinuousParameter\n",
    "from emukit.core.initial_designs.latin_design import LatinDesign\n",
    "\n",
    "# For statistical correlation analysis\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # suppress warnings carefully"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_min_count_epidemics(df, min_count=3):\n",
    "    \"\"\"\n",
    "    Filter for municipalities with at least `min_count` outbreaks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe containing epidemic records with column 'ocha_ID' (municipality code).\n",
    "    min_count : int\n",
    "        Minimum number of outbreaks required to keep a municipality.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Filtered dataframe containing only municipalities with at least `min_count` outbreaks.\n",
    "    \"\"\"\n",
    "    # Count outbreaks per municipality\n",
    "    outbreak_counts = df['ocha_ID'].value_counts()\n",
    "    \n",
    "    # Identify municipalities meeting threshold\n",
    "    valid_municipalities = outbreak_counts[outbreak_counts >= min_count].index\n",
    "    \n",
    "    # Filter dataframe\n",
    "    df_filtered = df[df['ocha_ID'].isin(valid_municipalities)].copy()\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def split_dataframe(df, train_proportion=0.67):\n",
    "    \"\"\"\n",
    "    Split dataframe into two subsets (train and test) while keeping outbreaks from\n",
    "    the same municipality grouped.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe containing epidemic records with column 'ocha_ID' (municipality code).\n",
    "    train_proportion : float\n",
    "        Proportion of outbreaks per municipality to include in the training set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of pd.DataFrame\n",
    "        df_train : DataFrame with `train_proportion` of outbreaks per municipality.\n",
    "        df_test : DataFrame with remaining outbreaks.\n",
    "    \"\"\"\n",
    "    df_train = pd.DataFrame()\n",
    "    df_test = pd.DataFrame()\n",
    "    \n",
    "    # Split data by municipality\n",
    "    for municipality, group in df.groupby('ocha_ID'):\n",
    "        num_outbreaks = len(group)\n",
    "        indices = np.arange(num_outbreaks)\n",
    "        np.random.shuffle(indices)  # Shuffle indices for random selection\n",
    "        \n",
    "        split_idx = int(train_proportion * num_outbreaks)\n",
    "        \n",
    "        df_train = pd.concat([df_train, group.iloc[indices[:split_idx]]], ignore_index=True)\n",
    "        df_test = pd.concat([df_test, group.iloc[indices[split_idx:]]], ignore_index=True)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def format_data(LHS_sample, epidemics, epidemic_id):\n",
    "    \"\"\"\n",
    "    Formats one epidemic's input data for GP prediction.\n",
    "\n",
    "    Each epidemic is characterized by its starting day ('start_day'),\n",
    "    which is used to shift the phase of the seasonality.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    LHS_sample : np.ndarray\n",
    "        Candidate parameter combinations (without correction factor column).\n",
    "    epidemics : pd.DataFrame\n",
    "        Epidemic metadata, must contain 'start_day'.\n",
    "    epidemic_id : int\n",
    "        Index of the epidemic being processed.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Formatted tensor ready for GP prediction.\n",
    "    \"\"\"\n",
    "    res = LHS_sample.copy()\n",
    "    start_day = epidemics.loc[epidemic_id, 'start_day']\n",
    "\n",
    "    # Apply phase shift adjustment (wrap around the seasonal cycle)\n",
    "    phase_shift = (res[:, 2] + start_day / 365.0) % 1.0\n",
    "    res[:, 2] = phase_shift\n",
    "\n",
    "    return torch.from_numpy(res).float().contiguous()\n",
    "\n",
    "\n",
    "def predict_var(LHS_sample, epidemics, GP_model, verbose=False):\n",
    "    \"\"\"\n",
    "    Performs GP predictions for each epidemic given a set of sampled parameters.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    LHS_sample : np.ndarray\n",
    "        Candidate parameter matrix (N_samples x N_params)\n",
    "    epidemics : pd.DataFrame\n",
    "        Epidemic data containing either 'max_incidence' or 'duration_days'\n",
    "    GP_model : SIR_GP\n",
    "        Trained GP emulator\n",
    "    verbose : bool\n",
    "        If True, prints progress messages\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Matrix of GP predictions [N_samples x N_epidemics]\n",
    "    \"\"\"\n",
    "    # Select observed variable type\n",
    "    if GP_model.model_type == \"maxIncidence\":\n",
    "        observed = epidemics['max_incidence'].values\n",
    "    elif GP_model.model_type == \"duration\":\n",
    "        observed = np.log10(epidemics['duration_days'].values)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown GP model type: {GP_model.model_type}\")\n",
    "\n",
    "    # Extract correction factor (last column)\n",
    "    correction = LHS_sample[:, -1].reshape(-1, 1)\n",
    "\n",
    "    n_samples = LHS_sample.shape[0]\n",
    "    n_epidemics = len(observed)\n",
    "    results = np.empty((n_samples, n_epidemics))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Predicting {n_samples} parameter sets across {n_epidemics} epidemics...\")\n",
    "\n",
    "    for i in range(n_epidemics):\n",
    "        if verbose and i % 25 == 0:\n",
    "            print(f\" → Processing epidemic {i+1}/{n_epidemics}\")\n",
    "\n",
    "        formatted_data = format_data(LHS_sample=LHS_sample[:, :-1],\n",
    "                                     epidemics=epidemics,\n",
    "                                     epidemic_id=i)\n",
    "\n",
    "        preds, _, _ = GP_model.predict_ys(parsed_data=formatted_data)\n",
    "\n",
    "        # Apply correction factor for maxIncidence\n",
    "        if GP_model.model_type == \"maxIncidence\":\n",
    "            results[:, i] = preds.numpy().flatten() * correction.flatten()\n",
    "        else:\n",
    "            results[:, i] = preds.numpy().flatten()\n",
    "\n",
    "    # Clip outputs to valid ranges\n",
    "    clip_bounds = (0.0, 1.0) if GP_model.model_type == \"maxIncidence\" else (0.0, 3.0)\n",
    "    return np.clip(results, *clip_bounds)\n",
    "\n",
    "def calculate_rmse(epidemics, predictions, model_type):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) between GP predictions and observed epidemic data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epidemics : pd.DataFrame\n",
    "        Empirical outbreak data for a set of epidemics. Must contain either 'max_incidence' or 'duration_days'.\n",
    "    predictions : np.ndarray\n",
    "        GP predictions with shape (n_candidates, n_epidemics)\n",
    "    model_type : str\n",
    "        Type of GP model. Options:\n",
    "        - \"maxIncidence\" : compare predicted vs observed maximum incidence\n",
    "        - \"duration\"     : compare predicted vs log10(observed duration_days)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        RMSE values for each candidate parameter set (shape: n_candidates,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract observed values based on model type\n",
    "    if model_type == \"maxIncidence\":\n",
    "        observed = epidemics['max_incidence'].values\n",
    "    elif model_type == \"duration\":\n",
    "        observed = np.log10(epidemics['duration_days'].values)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}. Must be 'maxIncidence' or 'duration'.\")\n",
    "    \n",
    "    # Ensure observed has shape (1, n_epidemics) for broadcasting\n",
    "    observed = observed.reshape(1, -1)\n",
    "    \n",
    "    # Compute RMSE for each candidate parameter set\n",
    "    rmse = np.sqrt(np.mean((predictions - observed) ** 2, axis=1))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "\n",
    "def shuffle_var(emp_df, f_alpha_df, f_candidate_LHS, n_iter, shuffle_var, GP_model):\n",
    "    \"\"\"\n",
    "    Conduct permutation tests by shuffling outbreak times, municipalities, or both.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    emp_df : pd.DataFrame\n",
    "        Empirical outbreak data.\n",
    "    f_alpha_df : pd.DataFrame\n",
    "        Municipality-specific alphaRest estimates.\n",
    "    f_candidate_LHS : np.ndarray\n",
    "        LHS parameter set used for predictions (excluding alphaRest).\n",
    "    n_iter : int\n",
    "        Number of permutation iterations.\n",
    "    shuffle_var : str\n",
    "        Type of shuffle: 'day', 'municipality', or 'both'.\n",
    "    GP_model : SIR_GP\n",
    "        Trained GP emulator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Spearman correlation coefficients from each permutation iteration.\n",
    "    \"\"\"\n",
    "    f_df = emp_df.copy()\n",
    "    shuffle_stats = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"{i} out of {n_iter} permutations done.\")\n",
    "\n",
    "        f_predictions = pd.DataFrame()\n",
    "\n",
    "        # Shuffle data based on specified method\n",
    "        if shuffle_var == 'day':\n",
    "            f_df['start_day'] = np.random.permutation(f_df['start_day'].values)\n",
    "        elif shuffle_var == 'municipality':\n",
    "            f_df['ocha_ID'] = np.random.permutation(f_df['ocha_ID'].values)\n",
    "        elif shuffle_var == 'both':\n",
    "            f_df['start_day'] = np.random.permutation(f_df['start_day'].values)\n",
    "            f_df['ocha_ID'] = np.random.permutation(f_df['ocha_ID'].values)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown shuffle_var: {shuffle_var}. Choose 'day', 'municipality', or 'both'.\")\n",
    "\n",
    "        # Loop over municipalities to make predictions\n",
    "        for m in f_alpha_df['municipality']:\n",
    "            # Subset empirical data for current municipality\n",
    "            f_df_m = f_df.loc[f_df['ocha_ID'] == m].reset_index(drop=True)\n",
    "\n",
    "            # Get municipality-specific alphaRest\n",
    "            f_alpha_m = f_alpha_df.loc[f_alpha_df['municipality'] == m, 'alphaRest'].values\n",
    "\n",
    "            # Combine alphaRest with LHS parameters\n",
    "            f_params = np.hstack((f_alpha_m, f_candidate_LHS)).reshape(1, -1)\n",
    "\n",
    "            # Predict max incidence\n",
    "            f_pred = predict_var(LHS_sample=f_params, epidemics=f_df_m, GP_model=GP_model)\n",
    "\n",
    "            # Combine predictions with original data\n",
    "            f_pred_df = pd.DataFrame(f_pred.flatten(), columns=['pred'])\n",
    "            f_combined_df = pd.concat([f_df_m, f_pred_df], axis=1)\n",
    "\n",
    "            # Append to cumulative predictions\n",
    "            f_predictions = pd.concat([f_predictions, f_combined_df], ignore_index=True)\n",
    "\n",
    "        # Compute Spearman correlation between observed and predicted\n",
    "        iter_stat = spearmanr(f_predictions['max_incidence'], f_predictions['pred']).statistic\n",
    "        shuffle_stats.append(iter_stat)\n",
    "\n",
    "    return shuffle_stats\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Parameter Setup\n",
    "\n",
    "Before calibrating the Gaussian Process (GP) emulator to empirical dengue outbreak data, we define the paths to all required datasets and the pre-trained GP model.  \n",
    "\n",
    "We also specify the ranges of model parameters to explore. For `alphaRest`, a separate small range is defined with discrete steps.  \n",
    "\n",
    "We configure sampling via Latin Hypercube Sampling (LHS) to generate candidate parameter combinations, set the proportion of data used for calibration (`PROP_FIT = 0.67`), and define output settings to retain the top parameter sets and perform permutations.\n",
    "\n",
    "**Note:** For demonstration purposes, the parameters are set to `N_SAMPLE_LHS = 1000`, `PARAMS_STEP_ALPHA = 10`, and `N_SHUFFLE = 100`.\n",
    "To reproduce analyses comparable to those presented in the [preprint](https://www.medrxiv.org/content/10.1101/2024.11.28.24318136v2), use `N_SAMPLE_LHS = 25000`, `PARAMS_STEP_ALPHA = 50`, and `N_SHUFFLE = 1000`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Paths to Data and Model Files\n",
    "# -------------------------------\n",
    "PATH_DATA = \"../data/empirical/OpenDengue_detected_epidemics.txt\"  # Empirical dengue outbreak summary\n",
    "PATH_GP_TRAIN = \"../data/GP/data/sim-training-maxIncidence-round15.txt\"  # GP training dataset\n",
    "PATH_GP_TEST = \"../data/GP/data/DD-AML-test-LHS-10000-condSim-logDuration.txt\"  # GP test dataset\n",
    "PATH_GP_MODEL = \"../data/GP/model/maxIncidence-round15-snap3.pth\"  # Pre-trained GP model snapshot\n",
    "\n",
    "# -------------------------------\n",
    "# Parameter Ranges for Exploration\n",
    "# -------------------------------\n",
    "PARAM_RANGES = ParameterSpace([\n",
    "    ContinuousParameter(\"alphaAmp\", 0, 1),\n",
    "    ContinuousParameter(\"alphaShift\", 0, 1),\n",
    "    ContinuousParameter(\"infTicksCount\", 4, 6),\n",
    "    ContinuousParameter(\"avgVisitsCount\", 1, 5),\n",
    "    ContinuousParameter(\"pVisits\", 0.05, 0.95),\n",
    "    ContinuousParameter(\"propSocialVisits\", 0, 1),\n",
    "    ContinuousParameter(\"locPerSGCount\", 1, 20),\n",
    "    ContinuousParameter(\"correctionFactor\", 0, 0.1)  # correction factor applied to incidences\n",
    "])\n",
    "\n",
    "# Parameter range for alphaRest (not included in PARAM_RANGES)\n",
    "PARAM_RANGES_ALPHA = [0, 0.03]  # Min and max values\n",
    "PARAM_STEPS_ALPHA = 10  # Number of steps to discretize alphaRest\n",
    "\n",
    "# -------------------------------\n",
    "# Sampling and Fitting Configuration\n",
    "# -------------------------------\n",
    "N_SAMPLE_LHS = 1000       # Number of parameter sets to draw from Latin Hypercube Sampling (LHS)\n",
    "RANDOM_STATE_SEED = 42    # Seed for reproducibility (LHS sample, train/test split)\n",
    "PROP_FIT = 0.67           # Proportion of empirical data used for GP calibration\n",
    "\n",
    "# -------------------------------\n",
    "# Output and Evaluation Settings\n",
    "# -------------------------------\n",
    "TOP_X = 250     # Keep top X parameter sets with lowest RMSE\n",
    "N_SHUFFLE = 100 # Number of iterations for permutation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Data Preparation\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "1. Load the detected dengue epidemics (`OpenDengue_detected_epidemics.txt`).\n",
    "2. Filter municipalities to only those with at least three outbreaks, ensuring sufficient data for calibration.\n",
    "3. Split the filtered data within each municipality into a calibration set (`df_calibration`, 67% of outbreaks) and a testing set (`df_testing`, 33% of outbreaks).\n",
    "4. Extract the list of unique municipalities used for GP calibration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered epidemics (municipalities with ≥3 outbreaks): (1186, 5)\n",
      "Calibration set (used for GP fitting): (737, 5)\n",
      "Testing set (used for prediction assessment): (449, 5)\n",
      "Number of unique municipalities in calibration set: 173\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Load and Prepare Empirical Epidemic Data\n",
    "# -------------------------------\n",
    "\n",
    "np.random.seed(RANDOM_STATE_SEED)  # Ensure reproducibility\n",
    "\n",
    "# Load detected dengue epidemics\n",
    "epidemics_all = pd.read_csv(PATH_DATA, sep=\"\\t\", header=0)\n",
    "\n",
    "# Filter to municipalities with at least 3 outbreaks\n",
    "epidemics_filtered = filter_min_count_epidemics(epidemics_all, min_count=3)\n",
    "\n",
    "# Split filtered data into calibration (fit) and testing (prediction) sets\n",
    "df_calibration, df_testing = split_dataframe(epidemics_filtered, train_proportion=PROP_FIT)\n",
    "\n",
    "# List of unique municipalities used for calibration\n",
    "unique_municipalities = df_calibration['ocha_ID'].unique()\n",
    "\n",
    "# -------------------------------\n",
    "# Display basic info\n",
    "# -------------------------------\n",
    "print(\"Filtered epidemics (municipalities with ≥3 outbreaks):\", epidemics_filtered.shape)\n",
    "print(\"Calibration set (used for GP fitting):\", df_calibration.shape)\n",
    "print(\"Testing set (used for prediction assessment):\", df_testing.shape)\n",
    "print(\"Number of unique municipalities in calibration set:\", len(unique_municipalities))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Generating Parameter Candidates\n",
    "\n",
    "To explore the parameter space efficiently, we generate a diverse set of parameter combinations using Latin Hypercube Sampling (LHS). Each sample represents a unique configuration of model parameters that will be evaluated using the trained GP emulator.\n",
    "\n",
    "We include two sets of parameters:\n",
    "\n",
    "* **GP parameter values, execpt alphaRest** are sampled with LHS.\n",
    "* **alphaRest values** are ampled over a linear range.\n",
    "\n",
    "The two sets are then combined into one array of parameter candidates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHS samples shape: (1000, 8)\n",
      "alphaRest range shape: (10, 1)\n",
      "Combined candidate matrix shape: (10000, 9)\n"
     ]
    }
   ],
   "source": [
    "# Generate candidate parameter combinations via Latin Hypercube Sampling (LHS)\n",
    "\n",
    "# Draw N_SAMPLE_LHS samples from the defined parameter ranges\n",
    "lhs_samples = LatinDesign(PARAM_RANGES).get_samples(N_SAMPLE_LHS)\n",
    "\n",
    "# Create a linearly spaced vector of alphaRest values\n",
    "alpha_rest_values = np.linspace(\n",
    "    PARAM_RANGES_ALPHA[0],\n",
    "    PARAM_RANGES_ALPHA[1],\n",
    "    PARAM_STEPS_ALPHA\n",
    ").reshape(-1, 1)\n",
    "\n",
    "# Combine LHS samples with alphaRest values:\n",
    "# - Each LHS sample is repeated for all alphaRest values\n",
    "# - The alphaRest vector is tiled to match the number of LHS samples\n",
    "lhs_repeated = np.repeat(lhs_samples, alpha_rest_values.shape[0], axis=0)\n",
    "alpha_tiled = np.tile(alpha_rest_values, (lhs_samples.shape[0], 1))\n",
    "\n",
    "# Final candidate matrix: alphaRest + all other parameters\n",
    "param_candidates = np.hstack((alpha_tiled, lhs_repeated))\n",
    "\n",
    "# Print dimensional summaries\n",
    "print(f\"LHS samples shape: {lhs_samples.shape}\")\n",
    "print(f\"alphaRest range shape: {alpha_rest_values.shape}\")\n",
    "print(f\"Combined candidate matrix shape: {param_candidates.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP\n",
    "### Load pre-trained GP\n",
    "In this section, we load the pre-trained Gaussian Process (GP) emulator for maximum dengue incidence.\n",
    "The model, trained earlier on individual-based simulations, serves as a fast surrogate for exploring the parameter space.\n",
    "\n",
    "**Note:** Loading the GP from disk requires that the GP is re-trained for a single iteration (`train(1)`) on the test data, whhich is done internally by the `.load()` function. This step can introduce very small stochastic differences between runs. Therefore, reported RMSE values or predictions may differ slightly between consecutive loads of the same model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Loss: -1.773698091506958\n",
      "Sanity check RMSE on test set: 0.0421\n"
     ]
    }
   ],
   "source": [
    "# === Load the pre-trained Gaussian Process (GP) emulator ===\n",
    "\n",
    "# Path to the trained model, training data, and test data are defined above.\n",
    "# The GP emulates the maximum dengue incidence (\"imax\") based on key model parameters.\n",
    "\n",
    "myGP = SIR_GP(training_data=PATH_GP_TRAIN, model_type=\"maxIncidence\")  # Initialize GP emulator\n",
    "myGP.load(filename=PATH_GP_MODEL)                                      # Load pre-trained weights\n",
    "\n",
    "# Perform a sanity check to verify that the loaded GP performs as expected\n",
    "rmse = myGP.get_rmse(PATH_GP_TEST)\n",
    "print(f\"Sanity check RMSE on test set: {rmse:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Here, we use the trained GP model to predict the maximum incidence for each epidemic in the calibration dataset across all sampled parameter sets.\n",
    "\n",
    "- `param_candidates`: LHS + alphaRest samples (each row = one parameter combination)\n",
    "- `df_calibration`: calibration epidemics (subset of empirical outbreaks)\n",
    "- Output: prediction matrix with shape [n_param_candidates × n_epidemics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 10000 parameter sets across 737 epidemics...\n",
      " → Processing epidemic 1/737\n",
      " → Processing epidemic 26/737\n",
      " → Processing epidemic 51/737\n",
      " → Processing epidemic 76/737\n",
      " → Processing epidemic 101/737\n",
      " → Processing epidemic 126/737\n",
      " → Processing epidemic 151/737\n",
      " → Processing epidemic 176/737\n",
      " → Processing epidemic 201/737\n",
      " → Processing epidemic 226/737\n",
      " → Processing epidemic 251/737\n",
      " → Processing epidemic 276/737\n",
      " → Processing epidemic 301/737\n",
      " → Processing epidemic 326/737\n",
      " → Processing epidemic 351/737\n",
      " → Processing epidemic 376/737\n",
      " → Processing epidemic 401/737\n",
      " → Processing epidemic 426/737\n",
      " → Processing epidemic 451/737\n",
      " → Processing epidemic 476/737\n",
      " → Processing epidemic 501/737\n",
      " → Processing epidemic 526/737\n",
      " → Processing epidemic 551/737\n",
      " → Processing epidemic 576/737\n",
      " → Processing epidemic 601/737\n",
      " → Processing epidemic 626/737\n",
      " → Processing epidemic 651/737\n",
      " → Processing epidemic 676/737\n",
      " → Processing epidemic 701/737\n",
      " → Processing epidemic 726/737\n",
      "Prediction matrix shape: (10000, 737)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = predict_var(\n",
    "    LHS_sample=param_candidates,\n",
    "    GP_model=myGP,\n",
    "    epidemics=df_calibration,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Prediction matrix shape: {predictions.shape}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "### Municipality-Specific RMSE Calculation\n",
    "\n",
    "This section calculates the Root Mean Squared Error (RMSE) for each candidate parameter set separately for each municipality, allowing us to identify which parameter combinations best reproduce observed outbreaks at the municipal level.\n",
    "\n",
    "**Key steps:**\n",
    "\n",
    "1. Iterate over each municipality.\n",
    "2. Identify indices in the calibration dataset corresponding to the current municipality.\n",
    "3. Compute RMSE between GP predictions and empirical data for that municipality.\n",
    "4. Store RMSE values in a matrix of shape `[n_candidates × n_municipalities]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE matrix shape: (10000, 173)\n"
     ]
    }
   ],
   "source": [
    "num_municipalities = len(unique_municipalities)      # Number of municipalities\n",
    "num_candidates = param_candidates.shape[0]          # Number of parameter sets\n",
    "rmse_municipality = np.empty((num_candidates, num_municipalities))  # Initialize RMSE matrix\n",
    "\n",
    "# Compute RMSE for each municipality\n",
    "for m_idx, municipality in enumerate(unique_municipalities):\n",
    "    # Indices of outbreaks corresponding to this municipality\n",
    "    municipality_indices = df_calibration.index[df_calibration['ocha_ID'] == municipality].tolist()\n",
    "    \n",
    "    # Calculate RMSE for this municipality across all candidate parameter sets\n",
    "    rmse_values = calculate_rmse(\n",
    "        epidemics=df_calibration.iloc[municipality_indices],\n",
    "        predictions=predictions[:, municipality_indices],\n",
    "        model_type=myGP.model_type\n",
    "    )\n",
    "    \n",
    "    rmse_municipality[:, m_idx] = rmse_values.flatten()\n",
    "\n",
    "print(f\"RMSE matrix shape: {rmse_municipality.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Optimal `alphaRest` per Municipality\n",
    "\n",
    "\n",
    "This section identifies the best `alphaRest` value within each block of candidate parameters for each municipality, based on the minimum RMSE.\n",
    "\n",
    "Key steps:\n",
    "\n",
    "1. Iterate over municipalities.\n",
    "2. For each block of parameter candidates that share the same non-`alphaRest` values, find the `alphaRest` value that minimizes RMSE.\n",
    "3. Store both the optimal `alphaRest` and corresponding minimum RMSE for each LHS sample and municipality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alphaRest matrix shape: (1000, 173)\n",
      "Minimum RMSE matrix shape: (1000, 173)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize arrays to store optimal alphaRest and corresponding RMSE\n",
    "alpha_municipalities = np.empty((N_SAMPLE_LHS, num_municipalities))\n",
    "min_rmse_municipalities = np.empty((N_SAMPLE_LHS, num_municipalities))\n",
    "\n",
    "# Loop over municipalities\n",
    "for m_idx in range(num_municipalities):\n",
    "    optimal_alpha = []  # Store best alphaRest per LHS sample\n",
    "    optimal_rmse = []   # Store corresponding RMSE\n",
    "\n",
    "    # Loop over each block of parameter candidates with the same LHS sample (different alphaRest values)\n",
    "    for i in range(0, num_candidates, PARAM_STEPS_ALPHA):\n",
    "        # Subset RMSE for the current block\n",
    "        rmse_block = rmse_municipality[i:(i + PARAM_STEPS_ALPHA), m_idx]\n",
    "\n",
    "        # Find index of minimum RMSE within block\n",
    "        min_idx = np.argmin(rmse_block)\n",
    "        min_rmse = rmse_block[min_idx]\n",
    "\n",
    "        # Store corresponding alphaRest value\n",
    "        alpha_value = alpha_rest_values[min_idx]\n",
    "        optimal_alpha.append(alpha_value)\n",
    "        optimal_rmse.append(min_rmse)\n",
    "\n",
    "    # Convert lists to arrays and store in final matrices\n",
    "    alpha_municipalities[:, m_idx] = np.array(optimal_alpha).flatten()\n",
    "    min_rmse_municipalities[:, m_idx] = np.array(optimal_rmse).flatten()\n",
    "\n",
    "print(f\"Optimal alphaRest matrix shape: {alpha_municipalities.shape}\")  # [LHS_samples × municipalities]\n",
    "print(f\"Minimum RMSE matrix shape: {min_rmse_municipalities.shape}\")     # [LHS_samples × municipalities]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Globally Best Parameter Set\n",
    "\n",
    "This step identifies the single best parameter set across all municipalities, based on the sum of minimum RMSEs per candidate parameter set. It also collects the corresponding optimal `alphaRest` values for each municipality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE sums shape: (1000,)\n",
      "LHS candidate matrix shape: (1000, 8)\n",
      "Best-fitting parameter set (excluding alphaRest):\n",
      "[[0.0075  0.3395  4.265   1.102   0.40145 0.5605  6.6145  0.02705]]\n",
      "        alphaRest\n",
      "count  173.000000\n",
      "mean     0.018555\n",
      "std      0.005594\n",
      "min      0.010000\n",
      "25%      0.013333\n",
      "50%      0.016667\n",
      "75%      0.020000\n",
      "max      0.030000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sum minimum RMSEs across municipalities for each LHS sample\n",
    "rmse_sums = np.sum(min_rmse_municipalities, axis=1)  # Shape: [n_LHS_samples]\n",
    "\n",
    "# Identify the index of the parameter set with the lowest total RMSE\n",
    "best_fit_id = np.argmin(rmse_sums)\n",
    "\n",
    "# Display summary\n",
    "print(f\"RMSE sums shape: {rmse_sums.shape}\")                   # [n_LHS_samples,]\n",
    "print(f\"LHS candidate matrix shape: {lhs_samples.shape}\")  # [n_LHS_samples × n_parameters (excluding alphaRest)]\n",
    "print(\"Best-fitting parameter set (excluding alphaRest):\")\n",
    "print(lhs_samples[best_fit_id:(best_fit_id+1), :])\n",
    "\n",
    "# Collect corresponding optimal alphaRest values for each municipality\n",
    "alpha_data = {\n",
    "    'municipality': unique_municipalities,\n",
    "    'alphaRest': alpha_municipalities[best_fit_id]\n",
    "}\n",
    "\n",
    "alpha_df = pd.DataFrame(alpha_data)\n",
    "print(alpha_df.describe())  # Summary statistics of alphaRest across municipalities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Top Parameter Sets\n",
    "\n",
    "This section identifies the TOP_X parameter sets with the lowest total RMSE across municipalities, providing a shortlist of best candidates for further analysis or reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 250 parameter sets shape: (250, 8)\n",
      "         alphaAmp  alphaShift  infTicksCount  avgVisitsCount     pVisits  \\\n",
      "count  250.000000  250.000000     250.000000      250.000000  250.000000   \n",
      "mean     0.390028    0.480280       4.921448        2.705008    0.491529   \n",
      "std      0.286588    0.292636       0.567150        1.161026    0.245857   \n",
      "min      0.000500    0.000500       4.001000        1.006000    0.052250   \n",
      "25%      0.138750    0.240750       4.475000        1.636000    0.281075   \n",
      "50%      0.332000    0.475000       4.907000        2.638000    0.483350   \n",
      "75%      0.608250    0.726000       5.313000        3.661000    0.699350   \n",
      "max      0.993500    0.999500       5.997000        4.990000    0.944150   \n",
      "\n",
      "       propSocialVisits  locPerSGCount  correctionFactor  \n",
      "count        250.000000     250.000000        250.000000  \n",
      "mean           0.538640      11.033064          0.024252  \n",
      "std            0.290985       5.459492          0.017556  \n",
      "min            0.001500       1.028500          0.004550  \n",
      "25%            0.296500       6.453000          0.012075  \n",
      "50%            0.562500      11.364500          0.019250  \n",
      "75%            0.795250      15.848500          0.030025  \n",
      "max            0.999500      19.990500          0.095350  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identify indices of TOP_X LHS samples with lowest RMSE sums\n",
    "top_indices = np.argsort(rmse_sums)[:TOP_X]  # Indices of top candidates\n",
    "\n",
    "# Extract corresponding parameter sets (excluding alphaRest)\n",
    "top_fits = lhs_samples[top_indices, :]  # Shape: [TOP_X × n_parameters]\n",
    "print(f\"Top {TOP_X} parameter sets shape: {top_fits.shape}\")\n",
    "\n",
    "# Convert to DataFrame for easier inspection and summary statistics\n",
    "top_fits_df = pd.DataFrame(top_fits, columns=PARAM_RANGES.parameter_names)\n",
    "print(top_fits_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Collecting `alphaRest` for Top Parameter Sets\n",
    "\n",
    "This section creates a DataFrame containing the`alphaRest` values per municipality for each of the TOP_X best parameter sets, along with their total RMSE and rank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          alphaRest          rmse          rank\n",
      "count  43250.000000  43250.000000  43250.000000\n",
      "mean       0.011004      0.704196    125.500000\n",
      "std        0.009310      0.049112     72.169041\n",
      "min        0.000000      0.598654      1.000000\n",
      "25%        0.003333      0.664435     63.000000\n",
      "50%        0.006667      0.707132    125.500000\n",
      "75%        0.013333      0.745508    188.000000\n",
      "max        0.030000      0.777695    250.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize DataFrame to store alphaRest info for TOP_X candidates\n",
    "alpha_df_topX = pd.DataFrame()\n",
    "\n",
    "# Loop over TOP_X best candidates\n",
    "for rank, idx in enumerate(top_indices, start=1):\n",
    "    alpha_data_iter = {\n",
    "        'municipality': unique_municipalities,\n",
    "        'alphaRest': alpha_municipalities[idx],\n",
    "        'rmse': rmse_sums[idx],\n",
    "        'rank': rank\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    alpha_df_iter = pd.DataFrame(alpha_data_iter)\n",
    "    \n",
    "    # Concatenate to cumulative DataFrame\n",
    "    alpha_df_topX = pd.concat([alpha_df_topX, alpha_df_iter], ignore_index=True)\n",
    "\n",
    "# Display summary statistics\n",
    "print(alpha_df_topX.describe())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Predictions\n",
    "\n",
    "In this section, we generate GP predictions for each municipality using the best-fitting parameters combined with municipality-specific `alphaRest` values.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Loop over municipalities.\n",
    "2. Extract municipality-specific epidemics from the prediction dataset.\n",
    "3. Combine the municipality’s `alphaRest` with the best-fitting LHS parameters.\n",
    "4. Predict maximum incidence using the GP model.\n",
    "5. Store predictions alongside the corresponding empirical data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions DataFrame shape: (449, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize DataFrame to store predictions\n",
    "my_predictions = pd.DataFrame()\n",
    "\n",
    "# Loop over municipalities\n",
    "for m in alpha_df['municipality']:\n",
    "    # Extract epidemics for the current municipality\n",
    "    df_pred_m = df_testing.loc[df_testing['ocha_ID'] == m].reset_index(drop=True)\n",
    "    \n",
    "    # Extract municipality-specific alphaRest\n",
    "    alpha_m = alpha_df.loc[alpha_df['municipality'] == m, 'alphaRest'].values  # shape (1,)\n",
    "    \n",
    "    # Combine alphaRest with best-fitting LHS parameters\n",
    "    pred_params = np.hstack((alpha_m, lhs_samples[best_fit_id]))  # shape (1, n_parameters + 1)\n",
    "    pred_params = pred_params.reshape(1, -1)  # ensure correct shape\n",
    "    \n",
    "    # Predict max incidence for this municipality\n",
    "    pred_m = predict_var(\n",
    "        LHS_sample=pred_params,\n",
    "        epidemics=df_pred_m,\n",
    "        GP_model=myGP,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Combine predictions with original data\n",
    "    pred_m_df = pd.DataFrame(pred_m.flatten(), columns=['pred'])\n",
    "    combined_df = pd.concat([df_pred_m, pred_m_df], axis=1)\n",
    "    \n",
    "    # Append to cumulative predictions DataFrame\n",
    "    my_predictions = pd.concat([my_predictions, combined_df], ignore_index=True)\n",
    "\n",
    "# Display shape of final predictions\n",
    "print(f\"Predictions DataFrame shape: {my_predictions.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Test for Model Validation\n",
    "\n",
    "This section evaluates the robustness of GP predictions by **permutation testing**, where the empirical data (i.e., starting day and municipality) are randomly shuffled. For each shuffle, predictions are recalculated, and the Spearman rank correlation between observed and predicted maximum incidences is stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 100 permutations done.\n",
      "10 out of 100 permutations done.\n",
      "20 out of 100 permutations done.\n",
      "30 out of 100 permutations done.\n",
      "40 out of 100 permutations done.\n",
      "50 out of 100 permutations done.\n",
      "60 out of 100 permutations done.\n",
      "70 out of 100 permutations done.\n",
      "80 out of 100 permutations done.\n",
      "90 out of 100 permutations done.\n"
     ]
    }
   ],
   "source": [
    "permut_both = shuffle_var(\n",
    "    emp_df=df_testing,\n",
    "    f_alpha_df=alpha_df,\n",
    "    f_candidate_LHS=lhs_samples[best_fit_id],\n",
    "    n_iter=N_SHUFFLE,\n",
    "    shuffle_var='both',\n",
    "    GP_model=myGP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Summarizing Permutation Test Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation test (both) summary:\n",
      "count    100.000000\n",
      "mean      -0.005019\n",
      "std        0.050506\n",
      "min       -0.142570\n",
      "25%       -0.041272\n",
      "50%       -0.009846\n",
      "75%        0.033469\n",
      "max        0.156240\n",
      "Name: both, dtype: float64\n",
      "Observed Spearman correlation: 0.4473\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DataFrame of permutation results\n",
    "permut_df = pd.DataFrame({\n",
    "    'shuffle_iteration': range(N_SHUFFLE),\n",
    "    'both': permut_both\n",
    "})\n",
    "\n",
    "# Summary statistics of permutation correlations\n",
    "print(\"Permutation test (both) summary:\")\n",
    "print(permut_df['both'].describe())\n",
    "\n",
    "# Compute Spearman correlation for the observed (unshuffled) predictions\n",
    "observed_corr = spearmanr(my_predictions['max_incidence'], my_predictions['pred']).statistic\n",
    "print(f\"Observed Spearman correlation: {observed_corr:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Export Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. GP predictions for all test epidemics\n",
    "my_predictions.to_csv(\n",
    "    'gp_predictions_calibration.tsv', \n",
    "    index=False, header=True, sep='\\t'\n",
    ")\n",
    "\n",
    "# 2. Best-fitting alphaRest parameters (per municipality, single best parameter set)\n",
    "alpha_df.to_csv(\n",
    "    'best_alphaRest_per_municipality.tsv', \n",
    "    index=False, header=True, sep='\\t'\n",
    ")\n",
    "\n",
    "# 3. Top X alphaRest parameters across municipalities (long-form for TOP_X candidates)\n",
    "alpha_df_topX.to_csv(\n",
    "    f'top{TOP_X}_alphaRest_per_municipality.tsv', \n",
    "    index=False, header=True, sep='\\t'\n",
    ")\n",
    "\n",
    "# 4. Top X best-fitting LHS parameter sets (excluding alphaRest)\n",
    "top_fits_df.to_csv(\n",
    "    f'top{TOP_X}_lhs_parameters.tsv', \n",
    "    index=False, header=True, sep='\\t'\n",
    ")\n",
    "\n",
    "# 5. Permutation test results (Spearman correlations)\n",
    "permut_df.to_csv(\n",
    "    'permutation_test_results.tsv', \n",
    "    index=False, header=True, sep='\\t'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPDummy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
